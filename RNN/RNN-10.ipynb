{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neruonal convolucional para detectar objetos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "session = tf.compat.v1.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables generales del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de muestras\n",
    "batch_size = 128\n",
    "\n",
    "# Salida del entrenamiento cada 50 pasos\n",
    "output_every_ = 50\n",
    "\n",
    "# Generaciones/pasos de entrenamiento\n",
    "generations = 20000\n",
    "\n",
    "# Evaluación de test cada 500 pasos\n",
    "eval_every = 500\n",
    "\n",
    "# Ancho y Alto de la imagen\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "\n",
    "# Cortes de imagenes\n",
    "crop_width = 24\n",
    "crop_height = 24\n",
    "\n",
    "# Numero de canales de color\n",
    "num_channels = 3\n",
    "\n",
    "# Numero de targets/objetivos\n",
    "num_targets = 10\n",
    "\n",
    "# Directorio donde encolaremos las imagenes\n",
    "data_folder = \"cifar-10-batches-bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radio de apredizaje variable\n",
    "\n",
    "$$learning\\ rate = 0.1\\cdot  0.9^{\\frac{x}{250}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "lr_decay = 0.9\n",
    "num_gens_to_wait = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prametros para lectura de CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño del vector para guardar la imagen\n",
    "image_vect_length = image_height*image_width*num_channels\n",
    "record_length = 1+image_vect_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga y procesamiento de CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yo ya tengo los ficheros descargados y descomprimidos\n",
    "data_dir = '../datasets/cifar-10-temp'\n",
    "\"\"\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = \"http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz\"\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if not os.path.isfile(data_file):\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file)\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distort_images = True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    _, record_string = reader.read(filename_queue)\n",
    "    #creamos fichero binario\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    #extraemos la etiqueta\n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "    #extraemos la imagen\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vect_length]), [num_channels, image_height, image_width])\n",
    "    #redimensión de imagen\n",
    "    reshaped_image = tf.transpose(image_extracted, [1,2,0])\n",
    "    reshaped_image = tf.cast(reshaped_image, tf.float32)\n",
    "    #crop aleatorio\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    if distort_images:\n",
    "        #flip aleatorio horizontal, cambiar brillo y contraste\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image, max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image, lower=0.2, upper=1.8)\n",
    "    #estandarización por color\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return final_image, image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, data_folder, 'data_batch_{}.bin'.format(i)) for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, data_folder, 'test_batch.bin')]\n",
    "    \n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    min_after_dequeue = 1000\n",
    "    capacity = min_after_dequeue+3*batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image,label], batch_size, capacity, min_after_dequeue)\n",
    "    return example_batch, label_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                               initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return tf.get_variable(name=name, shape=shape, dtype=dtype,\n",
    "                               initializer=tf.constant_initializer(0.0))\n",
    "    ## Primera capa de convolución\n",
    "    with tf.variable_scope(\"conv1\") as scope:\n",
    "        ##Conv de 5x5 para 3 canales con un total de 64 nodos al final\n",
    "        conv1_kernel = truncated_normal_var(name=\"conv_lernel1\", shape=[5,5,3,64], dtype=tf.float32)\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1,1,1,1],padding=\"SAME\")\n",
    "        conv1_bias = zero_var(name=\"conv_bias1\", shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        ## Capa de relu\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "    #Max pooling\n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\", name=\"pool_layer1\")\n",
    "    \n",
    "    #Normalización local\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm1\")\n",
    "    \n",
    "    ## Segunda capa de convolución\n",
    "    with tf.variable_scope(\"conv2\") as scope:\n",
    "        ##Conv de 5x5 para 64 nodos de entrada con un total de 64 nodos al final\n",
    "        conv2_kernel = truncated_normal_var(name=\"conv_lernel2\", shape=[5,5,64,64], dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1,1,1,1],padding=\"SAME\")\n",
    "        conv2_bias = zero_var(name=\"conv_bias2\", shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        ## Capa de relu\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "    #Max pooling\n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1,3,3,1], strides=[1,2,2,1],padding=\"SAME\", name=\"pool_layer2\")\n",
    "    \n",
    "    #Normalización local\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name=\"norm2\")\n",
    "    \n",
    "    #Redimensionar a una matriz para poder multiplicar en las capas totalmente conectadas\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size,-1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    ##Primera capa totalmente conectada\n",
    "    with tf.variable_scope(\"full1\") as scope:\n",
    "        #Full connected con 384 nodos\n",
    "        full_weight1 = truncated_normal_var(name=\"full_mult1\", shape=[reshaped_dim, 384], dtype = tf.float32)\n",
    "        full_bias1 = zero_var(name=\"full_bias1\", shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "    ##Segunda capa totalmente conectada\n",
    "    with tf.variable_scope(\"full2\") as scope:\n",
    "        #Full connected con 192 nodos\n",
    "        full_weight2 = truncated_normal_var(name=\"full_mult2\", shape=[384, 192], dtype = tf.float32)\n",
    "        full_bias2 = zero_var(name=\"full_bias2\", shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "    ##Tercera capa totalmente conectada\n",
    "    with tf.variable_scope(\"full3\") as scope:\n",
    "        #Full connected con una de las 10 categorías de salida\n",
    "        full_weight3 = truncated_normal_var(name=\"full_mult3\", shape=[192, num_targets], dtype = tf.float32)\n",
    "        full_bias3 = zero_var(name=\"full_bias3\", shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss_value, generation_num):\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num,\n",
    "                                                     num_gens_to_wait, lr_decay, staircase=True)\n",
    "    my_optim = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    train_step = my_optim.minimize(loss_value)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_from_batches(logits, targets):\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    batch_predictions = tf.cast(tf.argmax(logits,1), tf.int32)\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = input_pipeline(batch_size, train_logical=True)\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('model_definition') as scope:\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    scope.reuse_variables()\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cifar_loss(model_output, targets)\n",
    "accuracy = accuracy_from_batches(test_output, test_targets)\n",
    "generation_num = tf.Variable(0, trainable=False)\n",
    "train_op = train_step(loss, generation_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-input_producer-input_producer/input_producer_EnqueueMany, started daemon 140346428217088)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch/random_shuffle_queue-shuffle_batch/random_shuffle_queue_enqueue, started daemon 140346419824384)>,\n",
       " <Thread(QueueRunnerThread-input_producer_1-input_producer_1/input_producer_1_EnqueueMany, started daemon 140346411431680)>,\n",
       " <Thread(QueueRunnerThread-shuffle_batch_1/random_shuffle_queue-shuffle_batch_1/random_shuffle_queue_enqueue, started daemon 140346403038976)>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.start_queue_runners(sess=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 50, Pérdida: 1.85861\n",
      "Paso 100, Pérdida: 1.81485\n",
      "Paso 150, Pérdida: 1.71898\n",
      "Paso 200, Pérdida: 1.51860\n",
      "Paso 250, Pérdida: 1.55784\n",
      "Paso 300, Pérdida: 1.55024\n",
      "Paso 350, Pérdida: 1.50055\n",
      "Paso 400, Pérdida: 1.43909\n",
      "Paso 450, Pérdida: 1.69910\n",
      "Paso 500, Pérdida: 1.59071\n",
      "--- Precisión en test: 45.31%. ---\n",
      "Paso 550, Pérdida: 1.16671\n",
      "Paso 600, Pérdida: 1.39219\n",
      "Paso 650, Pérdida: 1.35201\n",
      "Paso 700, Pérdida: 1.32837\n",
      "Paso 750, Pérdida: 1.21907\n",
      "Paso 800, Pérdida: 1.36426\n",
      "Paso 850, Pérdida: 1.28788\n",
      "Paso 900, Pérdida: 1.29649\n",
      "Paso 950, Pérdida: 1.46739\n",
      "Paso 1000, Pérdida: 1.11758\n",
      "--- Precisión en test: 60.16%. ---\n",
      "Paso 1050, Pérdida: 1.17173\n",
      "Paso 1100, Pérdida: 0.92972\n",
      "Paso 1150, Pérdida: 1.26255\n",
      "Paso 1200, Pérdida: 1.10947\n",
      "Paso 1250, Pérdida: 0.94529\n",
      "Paso 1300, Pérdida: 0.93229\n",
      "Paso 1350, Pérdida: 1.19123\n",
      "Paso 1400, Pérdida: 1.05501\n",
      "Paso 1450, Pérdida: 1.00487\n",
      "Paso 1500, Pérdida: 0.89897\n",
      "--- Precisión en test: 63.28%. ---\n",
      "Paso 1550, Pérdida: 1.22240\n",
      "Paso 1600, Pérdida: 0.89672\n",
      "Paso 1650, Pérdida: 0.84929\n",
      "Paso 1700, Pérdida: 0.85479\n",
      "Paso 1750, Pérdida: 1.04756\n",
      "Paso 1800, Pérdida: 0.96521\n",
      "Paso 1850, Pérdida: 1.10354\n",
      "Paso 1900, Pérdida: 0.90045\n",
      "Paso 1950, Pérdida: 1.09052\n",
      "Paso 2000, Pérdida: 0.82916\n",
      "--- Precisión en test: 71.09%. ---\n",
      "Paso 2050, Pérdida: 0.73127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5236/1048535728.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0moutput_every_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_acc = []\n",
    "for i in range(generations):\n",
    "    _, loss_value = session.run([train_op, loss])\n",
    "    if (i+1) % output_every_ == 0:\n",
    "        train_loss.append(loss_value)\n",
    "        print(\"Paso {}, Pérdida: {:.5f}\".format((i+1),loss_value))\n",
    "    if (i+1) % eval_every == 0:\n",
    "        [temp_acc] = session.run([accuracy])\n",
    "        test_acc.append(temp_acc)\n",
    "        print(\"--- Precisión en test: {:.2f}%. ---\".format(100*temp_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
